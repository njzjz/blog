<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom"><title>DeepModeling</title><subtitle>Define the future of scientific computing together</subtitle><link href="https://deepmodeling.com/blog/atom.xml" rel="self"/><link href="https://deepmodeling.com/blog/"/><updated>2024-03-13T16:00:00.000Z</updated><id>https://deepmodeling.com/blog/</id><author><name>DeepModeling</name></author><generator uri="https://hexo.io/">Hexo</generator><entry><title>OpenLAM | Visualization and Analysis of Learned Representations in DPA-2: Encoding Chemical and Configurational Information</title><link href="https://deepmodeling.com/blog/OpenLAM-Visualization%20and%20Analysis%20of%20Learned%20Representations%20in%20DPA-2:%20Encoding%20Chemical%20and%20Configurational%20Information/"/><id>https://deepmodeling.com/blog/OpenLAM-Visualization%20and%20Analysis%20of%20Learned%20Representations%20in%20DPA-2:%20Encoding%20Chemical%20and%20Configurational%20Information/</id><published>2024-03-13T16:00:00.000Z</published><updated>2024-03-13T16:00:00.000Z</updated><content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>The slogan for OpenLAM is &quot;Conquer the Periodic Table!&quot; We hope to provide a new infrastructure for microscale scientific research and drive the transformation of microscale industrial design in fields such as materials, energy, and biopharmaceuticals by establishing an open-source ecosystem around large microscale models. Relevant models, data, and workflows will be consolidated around the AIS Square; related software development will take place in the DeepModeling open-source community. At the same time, we welcome open interaction from different communities in model development, data sharing, evaluation, and testing.</p><p>See <span class="exturl" data-url="aHR0cHM6Ly93d3cuYWlzc3F1YXJlLmNvbS9vcGVubGFt">AIS Square<i class="fa fa-external-link-alt"></i></span> for more details.</p><span id="more"></span><p>Recently, we revealed a remarkable correspondence between the learned representations by DPA-2 and existing chemical knowledge and the periodic table. The DPA-2 representation effectively distinguishes between various chemical and configurational environments, atoms sharing similar chemical and configurational environments are closer in the representation space learned by the DPA-2 model. It underscores the potential of the proposed model architecture and the multi-task training scheme.</p><p><img data-src="https://github.com/Chengqian-Zhang/blog/assets/100290172/c6feeb3b-1c91-4986-88f4-a7340e09e162" alt="image" loading="lazy"></p><p>We present a visualization of single-atom representations using a 2-dimensional t-SNE plot, as depicted in Fig.4. In Fig.4(a), colors denote distinct groups in the periodic table, as annotated in Fig.4(b). Notably, Fig.4(a) reveals that representations of identical chemical species tend to form cohesive clusters in the t-SNE latent space. The distribution of these representations distinctly aligns with known chemistry: The elements in groups IA and IIA are clustered at the top right of the t-SNE plot; The non-metals cluster predominantly at the top left and bottom; The transition metals, typically positioned at the middle of the periodic table, are accordingly situated in the central region of the t-SNE figure. However, hydrogen (H) presents an exception, exhibiting two clusters: one aligned with metals, primarily in water datasets, and another near non-metals, particularly in molecular datasets such as Drug, ANI-1x, and Transition-1x.</p><p>Elements such as Copper (Cu), Silver (Ag), and Gold (Au) in group IB exhibit a tendency to cluster closer to Lithium (Li) than other transition metals due to their shared possession of one s-electron in the outermost electron shell. Similarly, representations of group IIA elements like Calcium (Ca) and Strontium (Sr) closely associate with those of group IIB elements such as Zinc (Zn) and Cadmium (Cd) owing to their shared possession of two s-electrons in the outermost electron shell. Additionally, there&#39;s a discernible trend for elements from the same group in the periodic table to cluster together, as evident with Phosphorus (P), Arsenic (As), and Antimony (Sb) from group VII, and Selenium (Se) and Tellurium (Te) from group VIII.</p><p>The DPA-2 representation effectively distinguishes between various chemical and configurational environments, as showcased in Fig.4(c-e). In Fig.4(c), representations of Aluminum (Al) atoms from the Alloy and OC2M datasets are depicted. The color gradient from purple to yellow indicates the distance of the Al atom from the closest adsorbate in the OC2M dataset, while Al atoms from the Alloy dataset (all-metal environment) are colored red. Notably, Al atoms distanced from adsorbates closely resemble those in the Alloy dataset, indicative of similar chemical and configurational environments, whereas those in proximity to adsorbates exhibit discernible differences (see the red-circled blue cluster). Similarly, Fig.4(d) illustrates representations of Carbon (C) atoms in the Drug and OC2M datasets. Carbon atoms in adsorbates closer to catalyst materials are positioned farther away in latent space from representations in the Drug dataset due to more pronounced differences in their chemical and configurational environments.</p><p>Moreover, the DPA-2 representation shows insensitivity to DFT labeling accuracy. As demonstrated in Fig.4(e), representations of sulfur (S) in SSE-PBE (labeled with PBE exchange correlation functional) and SSE-PBESol (labeled with PBE-Sol exchange correlation functional) datasets exhibit mutual overlap. The S atoms form two clusters, with one cluster indicating a phosphorus neighboring atom and the other representing a neighboring Si&#x2F;Ge&#x2F;Sn atom.</p><p>In summary, our analysis reveals that atoms sharing similar chemical and configurational environments are closer in the representation space learned by the DPA-2 model. Thus, the DPA-2 representation emerges as a promising candidate for encoding chemical and configurational information in molecular and condensed-phase applications.</p>]]></content><summary type="html">&lt;p&gt;The slogan for OpenLAM is &amp;quot;Conquer the Periodic Table!&amp;quot; We hope to provide a new infrastructure for microscale scientific research and drive the transformation of microscale industrial design in fields such as materials, energy, and biopharmaceuticals by establishing an open-source ecosystem around large microscale models. Relevant models, data, and workflows will be consolidated around the AIS Square; related software development will take place in the DeepModeling open-source community. At the same time, we welcome open interaction from different communities in model development, data sharing, evaluation, and testing.&lt;/p&gt;
&lt;p&gt;See &lt;span class=&quot;exturl&quot; data-url=&quot;aHR0cHM6Ly93d3cuYWlzc3F1YXJlLmNvbS9vcGVubGFt&quot;&gt;AIS Square&lt;i class=&quot;fa fa-external-link-alt&quot;&gt;&lt;/i&gt;&lt;/span&gt; for more details.&lt;/p&gt;</summary><category term="OpenLAM" scheme="https://deepmodeling.com/blog/categories/OpenLAM/"/></entry><entry><title>OpenLAM | 2024 Q0 Report</title><link href="https://deepmodeling.com/blog/openlam-2024Q0/"/><id>https://deepmodeling.com/blog/openlam-2024Q0/</id><published>2024-01-09T16:00:00.000Z</published><updated>2024-01-09T16:00:00.000Z</updated><content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>The slogan for OpenLAM is &quot;Conquer the Periodic Table!&quot; We hope to provide a new infrastructure for microscale scientific research and drive the transformation of microscale industrial design in fields such as materials, energy, and biopharmaceuticals by establishing an open-source ecosystem around large microscale models. Relevant models, data, and workflows will be consolidated around the AIS Square; related software development will take place in the DeepModeling open-source community. At the same time, we welcome open interaction from different communities in model development, data sharing, evaluation, and testing.</p><p>See <span class="exturl" data-url="aHR0cHM6Ly93d3cuYWlzc3F1YXJlLmNvbS9vcGVubGFt">AIS Square<i class="fa fa-external-link-alt"></i></span> for more details.</p><span id="more"></span><h2 id="Model-Structure"><a href="#Model-Structure" class="headerlink" title="Model Structure"></a>Model Structure</h2><ul><li>The DPA-2 model structure (PyTorch based) has been released, showing a significant increase in fitting and transferability compared to the DPA-1 (<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzIzMTIuMTU0OTI=">arxiv:2312.15492<i class="fa fa-external-link-alt"></i></span>).</li><li>A new capability for unsupervised denoise pretraining has been added (<span class="exturl" data-url="aHR0cHM6Ly9kb2kub3JnLzEwLjUyODEvemVub2RvLjEwNDgzOTA4">DOI:10.5281&#x2F;zenodo.10483908<i class="fa fa-external-link-alt"></i></span>).</li></ul><h2 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h2><ul><li>The DPA-2 paper includes pretrained data for 18 systems and downstream data for 10 systems, covering over ten million frames and 73 elements (for detailed data inventory, see below; data can also be directly downloaded from <span class="exturl" data-url="aHR0cHM6Ly9kb2kub3JnLzEwLjUyODEvemVub2RvLjEwNDgzOTA4">DOI:10.5281&#x2F;zenodo.10483908<i class="fa fa-external-link-alt"></i></span>).</li><li>Four new datasets have been added for energy&amp;force data related to electrolytes, solid-state electrolytes, chemical reactions, and methane combustion (for details, see the data inventory below).</li><li>Seven new datasets in equilibrium state for unsupervised denoising tasks have been added, including AFLOW, MC2D&#x2F;3D, CALYPSO, etc. (for details, see the data inventory below).</li></ul><h2 id="Training-Strategy"><a href="#Training-Strategy" class="headerlink" title="Training Strategy"></a>Training Strategy</h2><ul><li>The DPA-2 paper includes a multi-task pretraining framework for energy and force, supporting the combined training of datasets with different DFT settings.</li><li>Unsupervised denoising task has been added, which is integrated into the multi-task pretraining framework (results are detailed below).</li></ul><h2 id="Automation-Process"><a href="#Automation-Process" class="headerlink" title="Automation Process"></a>Automation Process</h2><ul><li>The DPA-2 paper encompasses an automated process for all stages of pretraining, fine-tuning, transferability testing, distillation, and compression (experience it at <span class="exturl" data-url="aHR0cHM6Ly9hcHAuYm9ocml1bS5kcC50ZWNoL2RwLWNvbWJv">DP Combo<i class="fa fa-external-link-alt"></i></span> and try it on the <span class="exturl" data-url="aHR0cHM6Ly9uYi5ib2hyaXVtLmRwLnRlY2gvZGV0YWlsLzE4NDc1NDMzODI1">notebook<i class="fa fa-external-link-alt"></i></span>).</li><li>The <span class="exturl" data-url="aHR0cHM6Ly93d3cuYWlzc3F1YXJlLmNvbS9vcGVubGFt">AIS-Square website<i class="fa fa-external-link-alt"></i></span> now includes an automated process for integrating user data, automatically <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3pqZ2VtaS9sYW0tZGF0YS1jbGVhbmluZw==">determining the coverage<i class="fa fa-external-link-alt"></i></span> of the pretrained model on current data.</li></ul><h2 id="Competition"><a href="#Competition" class="headerlink" title="Competition"></a>Competition</h2><p>Coming in March...</p><h2 id="Teaching"><a href="#Teaching" class="headerlink" title="Teaching"></a>Teaching</h2><p>Coming in February...</p><p>Readers interested in the background of the project and details of the paper can also refer to the <span class="exturl" data-url="aHR0cHM6Ly9tcC53ZWl4aW4ucXEuY29tL3MvcWZYN1AzMmRLWi1WWmdPdGxZUnlfQQ==">OpenLAM initiative<i class="fa fa-external-link-alt"></i></span> and the <span class="exturl" data-url="aHR0cHM6Ly9tcC53ZWl4aW4ucXEuY29tL3MvcXlfdC1lSGRaNW5OZjRMOGkxc0tmUQ==">DPA-2 paper<i class="fa fa-external-link-alt"></i></span> for further information.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Since the release of DPA-2 less than a month ago, there have been numerous developments that can be summarized as follows:</p><ul><li>The DPA-2 multitask pre-training framework has added a new unsupervised training task: it is now possible to train with any data derived from different DFT calculations together, as well as denoise equilibrium state data without DFT labels, thereby learning a broader range of representation information;</li><li>The OpenLAM initiative has incorporated more production-type data and integrated more publicly available equilibrium state crystal structure data, with the pre-training data pool continuing to expand rapidly;</li><li>After incorporating the unsupervised training task, the overall energy prediction accuracy of the model is higher when compared fairly, indicating that information across different systems and tasks promotes mutual enhancement.</li></ul><p>The OpenLAM initiative is currently in rapid continuous iteration. As we move towards the era of large atomic models, open-source sharing becomes an inevitable theme. We welcome like-minded individuals to join, opening up new opportunities for broader scientific discoveries and industrial applications. On the journey to conquering the periodic table of elements, we look forward to creating a new era with you!<br>To join the &quot;OpenLAM Initiative&quot;, visit <span class="exturl" data-url="aHR0cHM6Ly93d3cuYWlzc3F1YXJlLmNvbS9vcGVubGFt">AISSquare<i class="fa fa-external-link-alt"></i></span>.</p><h2 id="Appendix"><a href="#Appendix" class="headerlink" title="Appendix"></a>Appendix</h2><ul><li>Unsupervised Denoise Method<ul><li>Data Structure<ul><li>Equilibrium state data consisting only of configurations without DFT computational results; noise is added separately to the coordinates and types during preprocessing (such as adding Gaussian noise to coordinate positions and masking certain element types).</li></ul></li><li>Training Method<ul><li>Configurations with added noise are inputted into the network, processed by DPA-2&#39;s unified descriptor and denoise fitting, to yield a denoise vector for each atom (i.e., the network&#39;s prediction of the proper displacement) as well as the element types. After restoring the configuration and element types based on the denoise vector, a loss is computed against the true configurations and element types without noise. The model is trained by minimizing this loss.</li></ul></li></ul></li><li>Data Inventory<br>The datasets currently used for training the DPA-2 model cover a wide range of systems including semiconductors, perovskites, alloys, surface catalysis, cathode materials, solid-state electrolytes, organic molecules, and more. This includes the newly added unsupervised equilibrium state Denoise datasets. All these data have been uploaded to the <span class="exturl" data-url="aHR0cHM6Ly93d3cuYWlzc3F1YXJlLmNvbS8=">AISSquare website<i class="fa fa-external-link-alt"></i></span>, where users can find more detailed data descriptions, as well as download and use the datasets, specifically including:<ul><li>Datasets included in the DPA-2 paper</li></ul></li></ul><table><thead><tr><th>Index</th><th>Dataset name</th><th>Contributors</th></tr></thead><tbody><tr><td>1</td><td>Alloy_DPA_v1_0</td><td>Fuzhi Dai, Wanrun Jiang</td></tr><tr><td>2</td><td>Cathode(Anode)_DPA_v1_0</td><td>Linshuang Zhang, Jianchuan Liu</td></tr><tr><td>3</td><td>Cluster_DPA_v1_0</td><td>Fuqiang Gong</td></tr><tr><td>4</td><td>Drug(drug-like-molecule)_DPA_v1_0</td><td>Manyi Yang</td></tr><tr><td>5</td><td>FerroEle_DPA_v1_0</td><td>Jing Wu, Jiyuan Yang, YuanJinsheng Liu, Duo Zhang, Yudi Yang, Yuzhi Zhang, Linfeng Zhang, Shi Liu</td></tr><tr><td>6</td><td>Open_Catalyst_2020(OC20_Dataset)</td><td>Duo Zhang</td></tr><tr><td>7</td><td>SSE-PBE_DPA_v1_0</td><td>Jianxing Huang</td></tr><tr><td>8</td><td>SemiCond_DPA_v1_0</td><td>Jianchuan Liu</td></tr><tr><td>9</td><td>H2O-PD_DPA_v1_0</td><td>Linfeng Zhang, Han Wang, Roberto Car, Weinan E</td></tr><tr><td>10</td><td>AgAu-PBE(unitary)_DPA_v1_0</td><td>Yinan Wang, LinFeng Zhang, Ben Xu, Xiaoyang Wang, Han Wang</td></tr><tr><td>11</td><td>AlMgCu_DPA_v1_0</td><td>Wanrun Jiang, Yuzhi Zhang, Linfeng Zhang, Han Wang</td></tr><tr><td>12</td><td>Cu_DPA_v1_0</td><td>Yuzhi Zhang, Haidi Wang, Weijie Chen, Jinzhe Zeng, Linfeng Zhang</td></tr><tr><td>13</td><td>Sn_DPA_v1_0</td><td>Fengbo Yuan</td></tr><tr><td>14</td><td>Ti_DPA_v1_0</td><td>Tongqi Wen, Rui Wang, Lingyu Zhu, Linfeng Zhang, Han Wang, David J Srolovitz, Zhaoxuan Wu</td></tr><tr><td>15</td><td>V_DPA_v1_0</td><td>Rui Wang, Xiaoxiao Ma, Linfeng Zhang, Han Wang, David J Srolovitz, Tongqi Wen, Zhaoxuan Wu</td></tr><tr><td>16</td><td>W_DPA_v1_0</td><td>Xiaoyang Wang, Yinan Wang, Linfeng Zhang, Fuzhi Dai, Han Wang</td></tr><tr><td>17</td><td>C12H26_DPA_v1_0</td><td>Jinzhe Zeng, Linfeng Zhang, Han Wang, Tong Zhu</td></tr><tr><td>18</td><td>HfO2_DPA_v1_0</td><td>Jing Wu, Yuzhi Zhang, Linfeng Zhang, Shi Liu</td></tr></tbody></table><ul><li>Four new datasets for energy &amp; force data</li></ul><table><thead><tr><th>Index</th><th>Dataset name</th><th>Contributors</th></tr></thead><tbody><tr><td>19</td><td>Electrolyte</td><td>Mengchao Shi, Yuzhi Zhang</td></tr><tr><td>20</td><td>Solid_State_Electrolyte</td><td>Mengchao Shi, Yuzhi Zhang</td></tr><tr><td>21</td><td>Organic_reactions_dataset</td><td>Tong Zhu, Bowen Li</td></tr><tr><td>22</td><td>CHO-methane-combustion</td><td>Jinzhe Zeng, Liqun Cao, Mingyuan Xu, Tong Zhu, John ZH Zhang</td></tr></tbody></table><ul><li>Seven new datasets in equilibrium state for unsupervised denoising</li></ul><table><thead><tr><th>Index</th><th>Dataset name</th><th>Contributors&#x2F;Link</th></tr></thead><tbody><tr><td>1</td><td>AFLOW_MP</td><td><span class="exturl" data-url="aHR0cHM6Ly93d3cuYWZsb3dsaWIub3JnLw==">AFLOW<i class="fa fa-external-link-alt"></i></span>, <span class="exturl" data-url="aHR0cHM6Ly9uZXh0LWdlbi5tYXRlcmlhbHNwcm9qZWN0Lm9yZy8=">MP<i class="fa fa-external-link-alt"></i></span></td></tr><tr><td>2</td><td>MC2D</td><td>Davide Campi, Nicolas Mounet, Marco Gibertini, Giovanni Pizzi, Nicola Marzari, <em>The Materials Cloud 2D database (MC2D)</em>, Materials Cloud Archive 2022.84 (2022), doi: <span class="exturl" data-url="aHR0cHM6Ly9kb2kub3JnLzEwLjI0NDM1L21hdGVyaWFsc2Nsb3VkOjM2LW5k">10.24435&#x2F;materialscloud:36-nd<i class="fa fa-external-link-alt"></i></span>.</td></tr><tr><td>3</td><td>MC3D</td><td>Sebastiaan Huber, Marnik Bercx, Nicolas Hörmann, Martin Uhrin, Giovanni Pizzi, Nicola Marzari, <em>Materials Cloud three-dimensional crystals database (MC3D)</em>, Materials Cloud Archive 2022.38 (2022), doi: <span class="exturl" data-url="aHR0cHM6Ly9kb2kub3JnLzEwLjI0NDM1L21hdGVyaWFsc2Nsb3VkOnJ3LXQw">10.24435&#x2F;materialscloud:rw-t0<i class="fa fa-external-link-alt"></i></span>.</td></tr><tr><td>4</td><td>ChemicalSimilarity</td><td>Hai-Chen Wang, Silvana Botti, Miguel A. L. Marques, <em>Finding new crystalline compounds using chemical similarity</em>, Materials Cloud Archive 2021.68 (2021), doi: <span class="exturl" data-url="aHR0cHM6Ly9kb2kub3JnLzEwLjI0NDM1L21hdGVyaWFsc2Nsb3VkOjk2LTA5">10.24435&#x2F;materialscloud:96-09<i class="fa fa-external-link-alt"></i></span>.</td></tr><tr><td>5</td><td>ClusterIsomer</td><td>Giuseppe Fisicaro, Bastian Schaefer, Jonas A. Finkler, Stefan Goedecker, <em>Principles of isomer stability in small clusters</em>, Materials Cloud Archive 2023.36 (2023), doi: <span class="exturl" data-url="aHR0cHM6Ly9kb2kub3JnLzEwLjI0NDM1L21hdGVyaWFsc2Nsb3VkOjQ2LW5y">10.24435&#x2F;materialscloud:46-nr<i class="fa fa-external-link-alt"></i></span>.</td></tr><tr><td>6</td><td>MolecularCrystal</td><td>Rose Cersonsky, Maria Pakhnova, Edgar Engel, Michele Ceriotti, <em>Lattice energies and relaxed geometries for 2&#39;707 organic molecular crystals and their 3&#39;242 molecular components.</em>, Materials Cloud Archive 2023.5 (2023), doi: <span class="exturl" data-url="aHR0cHM6Ly9kb2kub3JnLzEwLjI0NDM1L21hdGVyaWFsc2Nsb3VkOjcxLTIx">10.24435&#x2F;materialscloud:71-21<i class="fa fa-external-link-alt"></i></span>.</td></tr><tr><td>7</td><td>CALYPSO_database</td><td>Zhenyu Wang, Xiaoshan Luo</td></tr></tbody></table><ul><li>Latest Performance (root mean squared error, RMSE) of the Multi-task Pretrained Model (22 energy force systems + 7 unsupervised denoise systems)</li></ul><table>  <thead>    <tr>      <th rowspan=2></th>      <th rowspan=2>Weight</th>      <th colspan=2>DPA2 (multi-task 18 heads for 1m steps)</th>      <th colspan=2>DPA2 (multi-task 29 heads for 1.84m steps)</th>    </tr>    <tr>      <th>Energy (meV/atom)</td>      <th>Force  (meV/Å)</td>      <th>Energy (meV/atom)</td>      <th>Force (meV/Å)</td>    </tr>  </thead>  <tbody>    <tr>      <td>Alloy</td>      <td>2.0</td>      <td>36.5</td>      <td>169.5</td>      <td>32.2</td>      <td>160.5</td>    </tr>    <tr>      <td>Cluster</td>      <td>1.0</td>      <td>34.4</td>      <td>162.5</td>      <td>40.6</td>      <td>171.0</td>    </tr>    <tr>      <td>Anode</td>      <td>1.0</td>      <td>3.3</td>      <td>39.8</td>      <td>2.5</td>      <td>45.0</td>    </tr>    <tr>      <td>FerroEle</td>      <td>1.0</td>      <td>4.4</td>      <td>44.2</td>      <td>1.7</td>      <td>47.2</td>    </tr>    <tr>      <td>AgAu-PBE</td>      <td>0.2</td>      <td>9.4</td>      <td>28.2</td>      <td>10.9</td>      <td>31.2</td>    </tr>    <tr>      <td>Cu</td>      <td>0.1</td>      <td>3.6</td>      <td>18.2</td>      <td>6.8</td>      <td>21.2</td>    </tr>    <tr>      <td>Sn</td>      <td>0.1</td>      <td>24.8</td>      <td>69.7</td>      <td>17.3</td>      <td>76.7</td>    </tr>    <tr>      <td>Ti</td>      <td>0.1</td>      <td>16.3</td>      <td>112.4</td>      <td>26.8</td>      <td>133.7</td>    </tr>    <tr>      <td>AlMgCu</td>      <td>0.3</td>      <td>4.9</td>      <td>23.4</td>      <td>10.6</td>      <td>28.6</td>    </tr>    <tr>      <td>V</td>      <td>0.1</td>      <td>13.9</td>      <td>110.2</td>      <td>16.7</td>      <td>121.3</td>    </tr>    <tr>      <td>W</td>      <td>0.1</td>      <td>24.6</td>      <td>157.9</td>      <td>45.8</td>      <td>174.0</td>    </tr>    <tr>      <td>C12H26</td>      <td>0.1</td>      <td>62.5</td>      <td>710.6</td>      <td>75.3</td>      <td>1486.7</td>    </tr>    <tr>      <td>SSE-PBE</td>      <td>1.0</td>      <td>2.1</td>      <td>64.0</td>      <td>2.2</td>      <td>75.7</td>    </tr>    <tr>      <td>HfO2</td>      <td>0.1</td>      <td>3.9</td>      <td>102.8</td>      <td>5.0</td>      <td>108.4</td>    </tr>    <tr>      <td>SemiCond</td>      <td>1.0</td>      <td>6.5</td>      <td>131.9</td>      <td>7.2</td>      <td>139.8</td>    </tr>    <tr>      <td>Drug</td>      <td>2.0</td>      <td>20.6</td>      <td>128.9</td>      <td>21.8</td>      <td>140.6</td>    </tr>    <tr>      <td>OC2M</td>      <td>2.0</td>      <td>29.3</td>      <td>157.6</td>      <td>26.7</td>      <td>138.7</td>    </tr>    <tr>      <td>H2O-PD</td>      <td>1.0</td>      <td>3.2</td>      <td>39.7</td>      <td>1.0</td>      <td>45.6</td>    </tr>    <tr>      <td><strong>Weighted sum</strong></td>      <td></td>      <td><strong>18.6</strong></td>      <td><strong>116.3</strong></td>      <td><strong>18.3</strong></td>      <td><strong>123.6</strong></td>    </tr>    <tr>      <td></td>      <td></td>      <td></td>      <td></td>      <td></td>      <td></td>    </tr>    <tr>      <td>Electrolyte</td>      <td>1.0</td>      <td>/</td>      <td>/</td>      <td>2.9</td>      <td>64.3</td>    </tr>    <tr>      <td>SSE_new</td>      <td>1.0</td>      <td>/</td>      <td>/</td>      <td>3.2</td>      <td>72.4</td>    </tr>    <tr>      <td>Organic_reactions</td>      <td>1.0</td>      <td>/</td>      <td>/</td>      <td>15.1</td>      <td>97.7</td>    </tr>    <tr>      <td>Methane-combustion</td>      <td>1.0</td>      <td>/</td>      <td>/</td>      <td>147.2</td>      <td>251.4</td>    </tr>  </tbody></table>]]></content><summary type="html">&lt;p&gt;The slogan for OpenLAM is &amp;quot;Conquer the Periodic Table!&amp;quot; We hope to provide a new infrastructure for microscale scientific research and drive the transformation of microscale industrial design in fields such as materials, energy, and biopharmaceuticals by establishing an open-source ecosystem around large microscale models. Relevant models, data, and workflows will be consolidated around the AIS Square; related software development will take place in the DeepModeling open-source community. At the same time, we welcome open interaction from different communities in model development, data sharing, evaluation, and testing.&lt;/p&gt;
&lt;p&gt;See &lt;span class=&quot;exturl&quot; data-url=&quot;aHR0cHM6Ly93d3cuYWlzc3F1YXJlLmNvbS9vcGVubGFt&quot;&gt;AIS Square&lt;i class=&quot;fa fa-external-link-alt&quot;&gt;&lt;/i&gt;&lt;/span&gt; for more details.&lt;/p&gt;</summary><category term="OpenLAM" scheme="https://deepmodeling.com/blog/categories/OpenLAM/"/></entry><entry><title>The OpenLAM Initiative</title><link href="https://deepmodeling.com/blog/openlam/"/><id>https://deepmodeling.com/blog/openlam/</id><published>2023-11-30T16:00:00.000Z</published><updated>2023-11-30T16:00:00.000Z</updated><content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>Peter Thiel once said, &quot;We wanted flying cars, instead we got 140 characters (Twitter).&quot; Over the past decade, we have made great strides at the bit level (internet), but progress at the atomic level (cutting-edge technology) has been relatively slow.</p><p>The accumulation of linguistic data has propelled the development of machine learning and ultimately led to the emergence of Large Language Models (LLMs). With the push from AI, progress at the atomic level is also accelerating. Methods like Deep Potential, by learning quantum mechanical data, have increased the space-time scale of microscopic simulations by several orders of magnitude and have made significant progress in fields like drug design, material design, and chemical engineering.</p><p>The accumulation of quantum mechanical data is gradually covering the entire periodic table, and the Deep Potential team has also begun the practice of the <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzIyMDguMDgyMzY=">DPA pre-training model<i class="fa fa-external-link-alt"></i></span>. Analogous to the progress of LLMs, we are on the eve of the emergence of a general Large Atom Model (LAM). At the same time, we believe that open-source and openness will play an increasingly important role in the development of LAM.</p><p>Against this backdrop, the core developer team of Deep Potential is launching the OpenLAM Initiative to the community. This plan is still in the draft stage and is set to officially start on January 1, 2024. We warmly and openly welcome opinions and support from all parties.</p><p>The slogan for OpenLAM is &quot;Conquer the Periodic Table!&quot; We hope to provide a new infrastructure for microscale scientific research and drive the transformation of microscale industrial design in fields such as materials, energy, and biopharmaceuticals by establishing an open-source ecosystem around large microscale models. Relevant models, data, and workflows will be consolidated around the <span class="exturl" data-url="aHR0cHM6Ly93d3cuYWlzc3F1YXJlLmNvbS8=">AIS Square<i class="fa fa-external-link-alt"></i></span>; related software development will take place in the DeepModeling open-source community. At the same time, we welcome open interaction from different communities in model development, data sharing, evaluation, and testing.</p><p>OpenLAM&#39;s goals for the next three years are: In 2024, to effectively cover the periodic table with first-principles data and achieve a universal property learning capability; in 2025, to combine large-scale experimental characterization data and literature data to achieve a universal cross-modal capability; and in 2026, to realize a target-oriented atomic scale universal generation and planning capability. Ultimately, within 5-10 years, we aim to achieve &quot;Large Atom Embodied Intelligence&quot; for atomic-scale intelligent scientific discovery and synthetic design.</p><p>OpenLAM&#39;s specific plans for 2024 include:</p><ul><li><p>Model Update and Evaluation Report Release:</p><ul><li>Starting from January 1, 2024, driven by the Deep Potential team, with participation from all LAM developers welcomed.</li><li>Every three months, a major model version update will take place, with updates that may include model architecture, related data, training strategies, and evaluation test criteria.</li></ul></li><li><p>AIS Cup Competition:</p><ul><li>Initiated by the Deep Potential team and supported by the <span class="exturl" data-url="aHR0cHM6Ly9ib2hyaXVtLmRwLnRlY2gv">Bohrium Cloud Platform<i class="fa fa-external-link-alt"></i></span>, starting in March 2024 and concluding at the end of the year;</li><li>The goal is to promote the creation of a benchmarking system focused on several application-oriented metrics.</li></ul></li><li><p>Domain Data Contribution:</p><ul><li>Seeking collaboration with domain developers to establish &quot;LAM-ready&quot; datasets for pre-training and evaluation.</li><li>Domain datasets for iterative training of the latest models will be updated every three months.</li></ul></li><li><p>Domain Application and Evaluation Workflow Contribution:</p><ul><li>The domain application and evaluation workflows will be updated and released every three months.</li></ul></li><li><p>Education and Training:</p><ul><li>Planning a series of educational and training events aimed at LAM developers, domain developers, and users to encourage advancement in the field.</li></ul></li><li><p>How to Contact Us:</p><ul><li>Direct discussions are encouraged in the <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2RlZXBtb2RlbGluZy9jb21tdW5pdHkvZGlzY3Vzc2lvbnMvMzI=">DeepModeling community<i class="fa fa-external-link-alt"></i></span>.</li><li>For more complex inquiries, please contact the project lead, Han Wang (王涵, <span class="exturl" data-url="bWFpbHRvOndhbmdfaGFuQGlhcGNtLmFjLmNu">wang_han@iapcm.ac.cn<i class="fa fa-external-link-alt"></i></span>), Linfeng Zhang (张林峰, <span class="exturl" data-url="bWFpbHRvOnpoYW5nbGZAYWlzaS5hYy5jbg==">zhanglf@aisi.ac.cn<i class="fa fa-external-link-alt"></i></span>), for the new future of Science!</li></ul></li></ul>]]></content><summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text&amp;#x2F;css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;p&gt;Peter Thiel once said, &amp;quot;We w</summary><category term="OpenLAM" scheme="https://deepmodeling.com/blog/categories/OpenLAM/"/></entry><entry><title>2022 CSI Workshop: Deep Modeling for Molecular Simulation</title><link href="https://deepmodeling.com/blog/2022_csi_workshop/"/><id>https://deepmodeling.com/blog/2022_csi_workshop/</id><published>2022-07-07T16:00:00.000Z</published><updated>2022-07-07T16:00:00.000Z</updated><content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="Lecture-1-Deep-Potential-Method-for-Molecular-Simulation-Roberto-Car"><a href="#Lecture-1-Deep-Potential-Method-for-Molecular-Simulation-Roberto-Car" class="headerlink" title="Lecture 1: Deep Potential Method for Molecular Simulation, Roberto Car"></a>Lecture 1: Deep Potential Method for Molecular Simulation, Roberto Car</h2><!--begin_84c46d02de7aeec7c0ad2f3ce7df8aa6--><div class="bilitube video-container" data-youtube="7JuIC37dx3E" data-bvid="BV1AT411J7RD"></div><!--end_84c46d02de7aeec7c0ad2f3ce7df8aa6--><h2 id="Lecture-2-Deep-Potential-at-Scale-Linfeng-Zhang"><a href="#Lecture-2-Deep-Potential-at-Scale-Linfeng-Zhang" class="headerlink" title="Lecture 2: Deep Potential at Scale, Linfeng Zhang"></a>Lecture 2: Deep Potential at Scale, Linfeng Zhang</h2><!--begin_84c46d02de7aeec7c0ad2f3ce7df8aa6--><div class="bilitube video-container" data-youtube="iwuHzPPNucU" data-bvid="BV1LS4y1J7HW"></div><!--end_84c46d02de7aeec7c0ad2f3ce7df8aa6--><h2 id="Lecture-3-Towards-a-Realistic-Description-of-H3O-and-OH-Transport-Robert-A-DiStasio-Jr"><a href="#Lecture-3-Towards-a-Realistic-Description-of-H3O-and-OH-Transport-Robert-A-DiStasio-Jr" class="headerlink" title="Lecture 3: Towards a Realistic Description of H3O+ and OH- Transport, Robert A. DiStasio Jr."></a>Lecture 3: Towards a Realistic Description of H3O+ and OH- Transport, Robert A. DiStasio Jr.</h2><!--begin_84c46d02de7aeec7c0ad2f3ce7df8aa6--><div class="bilitube video-container" data-youtube="CDJTp1Ekn6I" data-bvid="BV1Ma411n7Gz"></div><!--end_84c46d02de7aeec7c0ad2f3ce7df8aa6--><h2 id="Lecture-4-Next-Generation-Quantum-and-Deep-Learning-Potentials-Darrin-York"><a href="#Lecture-4-Next-Generation-Quantum-and-Deep-Learning-Potentials-Darrin-York" class="headerlink" title="Lecture 4: Next Generation Quantum and Deep Learning Potentials, Darrin York"></a>Lecture 4: Next Generation Quantum and Deep Learning Potentials, Darrin York</h2><!--begin_84c46d02de7aeec7c0ad2f3ce7df8aa6--><div class="bilitube video-container" data-youtube="cULCSW1RJ6w" data-bvid="BV16V4y1n7La"></div><!--end_84c46d02de7aeec7c0ad2f3ce7df8aa6--><h2 id="Lecture-5-Linear-Response-Theory-of-Transport-in-Condensed-Matter-Stefano-Baroni"><a href="#Lecture-5-Linear-Response-Theory-of-Transport-in-Condensed-Matter-Stefano-Baroni" class="headerlink" title="Lecture 5: Linear Response Theory of Transport in Condensed Matter, Stefano Baroni"></a>Lecture 5: Linear Response Theory of Transport in Condensed Matter, Stefano Baroni</h2><!--begin_84c46d02de7aeec7c0ad2f3ce7df8aa6--><div class="bilitube video-container" data-youtube="jW5E70laCdQ" data-bvid="BV1xe4y1R7y9"></div><!--end_84c46d02de7aeec7c0ad2f3ce7df8aa6--><h2 id="Lecture-6-Deep-Modeling-with-Long-Range-Electrostatic-Interactions-Chunyi-Zhang"><a href="#Lecture-6-Deep-Modeling-with-Long-Range-Electrostatic-Interactions-Chunyi-Zhang" class="headerlink" title="Lecture 6: Deep Modeling with Long-Range Electrostatic Interactions, Chunyi Zhang"></a>Lecture 6: Deep Modeling with Long-Range Electrostatic Interactions, Chunyi Zhang</h2><!--begin_84c46d02de7aeec7c0ad2f3ce7df8aa6--><div class="bilitube video-container" data-youtube="ytLzXgO1N_M" data-bvid="BV1Gt4y147rK"></div><!--end_84c46d02de7aeec7c0ad2f3ce7df8aa6--><h2 id="Hands-on-session-4-Machine-learning-of-Wannier-centers-and-dipoles"><a href="#Hands-on-session-4-Machine-learning-of-Wannier-centers-and-dipoles" class="headerlink" title="Hands-on session 4: Machine learning of Wannier centers and dipoles"></a>Hands-on session 4: Machine learning of Wannier centers and dipoles</h2><!--begin_84c46d02de7aeec7c0ad2f3ce7df8aa6--><div class="bilitube video-container" data-youtube="0iDvZimtBQ4" data-bvid="BV1JG411W7tw"></div><!--end_84c46d02de7aeec7c0ad2f3ce7df8aa6--><h2 id="Hands-on-session-5-Long-range-electrostatic-interactions-with-DPLR"><a href="#Hands-on-session-5-Long-range-electrostatic-interactions-with-DPLR" class="headerlink" title="Hands-on session 5: Long range electrostatic interactions with DPLR"></a>Hands-on session 5: Long range electrostatic interactions with DPLR</h2><!--begin_84c46d02de7aeec7c0ad2f3ce7df8aa6--><div class="bilitube video-container" data-youtube="j6SuMXjaXuo" data-bvid="BV1ka411D78S"></div><!--end_84c46d02de7aeec7c0ad2f3ce7df8aa6--><h2 id="Hands-on-session-6-Concurrent-learning-with-DP-GEN"><a href="#Hands-on-session-6-Concurrent-learning-with-DP-GEN" class="headerlink" title="Hands-on session 6: Concurrent learning with DP-GEN"></a>Hands-on session 6: Concurrent learning with DP-GEN</h2><!--begin_84c46d02de7aeec7c0ad2f3ce7df8aa6--><div class="bilitube video-container" data-youtube="fVXTZQx38gk" data-bvid="BV1JG411W7mT"></div><!--end_84c46d02de7aeec7c0ad2f3ce7df8aa6-->]]></content><summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text&amp;#x2F;css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;h2 id=&quot;Lecture-1-Deep-Potential-Met</summary></entry><entry><title>DP Tutorial 2: DeePMD-kit: Install with Conda &amp; Offline Packages &amp; Docker</title><link href="https://deepmodeling.com/blog/tutorial2/"/><id>https://deepmodeling.com/blog/tutorial2/</id><published>2021-07-05T16:00:00.000Z</published><updated>2021-07-05T16:00:00.000Z</updated><content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>Do you prepare to read a long article before clicking the tutorial? Since we can teach you how to setup a DeePMD-kit training in 5 minutes, we can also teach you how to install DeePMD-kit in 5 minutes. The installation manual will be introduced as follows:</p><h2 id="Install-with-conda"><a href="#Install-with-conda" class="headerlink" title="Install with conda"></a>Install with conda</h2><p>After you install <code>conda</code>, you can install the CPU version with the following command:</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install deepmd-kit=*=*cpu lammps-dp=*=*cpu -c deepmodeling</span><br></pre></td></tr></table></figure><p>To install the GPU version containing CUDA 10.1:</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install deepmd-kit=*=*gpu lammps-dp=*=*gpu -c deepmodeling</span><br></pre></td></tr></table></figure><p>If you want to use the specific version, just replace <code>*</code> with the version:</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install deepmd-kit=1.3.3=*cpu lammps-dp=1.3.3=*cpu -c deepmodeling</span><br></pre></td></tr></table></figure><h2 id="Install-with-offline-packages"><a href="#Install-with-offline-packages" class="headerlink" title="Install with offline packages"></a>Install with offline packages</h2><p>Download offline packages in the <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2RlZXBtb2RlbGluZy9kZWVwbWQta2l0L3JlbGVhc2Vz">Releases page<i class="fa fa-external-link-alt"></i></span>, or use <code>wget</code>:</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https://github.com/deepmodeling/deepmd-kit/releases/download/v1.3.3/deepmd-kit-1.3.3-cuda10.1_gpu-Linux-x86_64.sh -O deepmd-kit-1.3.3-cuda10.1_gpu-Linux-x86_64.sh</span><br></pre></td></tr></table></figure><p>Take an example of <code>v1.3.3</code>. Execuate the following commands and just follow the prompts.</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh deepmd-kit-1.3.1-cuda10.1_gpu-Linux-x86_64.sh</span><br></pre></td></tr></table></figure><h2 id="With-Docker"><a href="#With-Docker" class="headerlink" title="With Docker"></a>With Docker</h2><p>To pull the CPU version:</p><p>docker pull ghcr.io&#x2F;deepmodeling&#x2F;deepmd-kit:1.2.2_cpu<br>To pull the GPU version:</p><p>docker pull ghcr.io&#x2F;deepmodeling&#x2F;deepmd-kit:1.2.2_cuda10.1_gpu</p><h2 id="Tips"><a href="#Tips" class="headerlink" title="Tips"></a>Tips</h2><p><code>dp</code> is the program of DeePMD-kit and <code>lmp</code> is the program of LAMMPS.</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dp -h</span><br><span class="line">lmp -h</span><br></pre></td></tr></table></figure><p>GPU version has contained CUDA Toolkit. Note that different CUDA versions support different NVIDIA driver versions. See <span class="exturl" data-url="aHR0cHM6Ly9kb2NzLm52aWRpYS5jb20vZGVwbG95L2N1ZGEtY29tcGF0aWJpbGl0eS8=">NVIDIA documents<i class="fa fa-external-link-alt"></i></span> for details.</p><p>Don&#39;t hurry up and try such a convenient installation process. But I still want to remind everyone that the above installation methods only support the official version released by DeePMD-kit. If you need to use the devel version, you still need to go through a long compilation process. Please refer to the <span class="exturl" data-url="aHR0cHM6Ly9kb2NzLmRlZXBtb2RlbGluZy5vcmcvcHJvamVjdHMvZGVlcG1kLw==">installation manual<i class="fa fa-external-link-alt"></i></span>.</p>]]></content><summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text&amp;#x2F;css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;p&gt;Do you prepare to read a long art</summary><category term="tutorial" scheme="https://deepmodeling.com/blog/categories/tutorial/"/><category term="DeePMD-kit" scheme="https://deepmodeling.com/blog/tags/DeePMD-kit/"/></entry><entry><title>DP Tutorial 1: How to Setup a DeePMD-kit Training within 5 Minutes?</title><link href="https://deepmodeling.com/blog/tutorial1/"/><id>https://deepmodeling.com/blog/tutorial1/</id><published>2021-06-11T16:00:00.000Z</published><updated>2021-06-11T16:00:00.000Z</updated><content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>DeePMD-kit is a software to implement Deep Potential. There is a lot of information on the Internet, but there are not so many tutorials for the new hand, and the official guide is too long. Today, I&#39;ll take you 5 minutes to get started with DeePMD-kit. </p><p>Let&#39;s take a look at the training process of DeePMD-kit:</p><pre class="mermaid">graph LRA[Prepare data] --&gt; B[Training]B --&gt; C[Freeze the model]</pre><p>What? Only three steps? Yes, it&#39;s that simple.<span id="more"></span> </p><ol><li><em><strong>Preparing data</strong></em> is converting the computational results of DFT to data that can be recognized by the DeePMD-kit. </li><li><em><strong>Training</strong></em> is train a Deep Potential model using the DeePMD-kit with data prepared in the previous step. </li><li>Finally, what we need to do is to <em><strong>freeze the restart file in the training process into a model</strong></em>, in other words is to extract the neural network parameters into a file for subsequent use. I believe you can&#39;t wait to get started. Let&#39;s go!</li></ol><h2 id="1-Preparing-Data"><a href="#1-Preparing-Data" class="headerlink" title="1. Preparing Data"></a>1. Preparing Data</h2><p>The data format of the DeePMD-kit is introduced in the <span class="exturl" data-url="aHR0cHM6Ly9kZWVwbWQucmVhZHRoZWRvY3MuaW8v">official document<i class="fa fa-external-link-alt"></i></span> but seems complex. Don&#39;t worry, I&#39;d like to introduce a data processing tool: <em><strong>dpdata</strong></em>! You can use only one line Python scripts to process data. So easy!</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> dpdata</span><br><span class="line">dpdata.LabeledSystem(<span class="string">&#x27;OUTCAR&#x27;</span>).to(<span class="string">&#x27;deepmd/npy&#x27;</span>, <span class="string">&#x27;data&#x27;</span>, set_size=<span class="number">200</span>)</span><br></pre></td></tr></table></figure><p>In this example, we converted the computational results of the VASP in the <code>OUTCAR</code> to the data format of the DeePMD-kit and saved in to a directory named <code>data</code>, where <code>npy</code> is the compressed format of the numpy, which is required by the DeePMD-kit training. We assume <code>OUTCAR</code> stores 1000 frames of molecular dynamics trajectory, then where will be 1000 points after converting. <code>set_size=200</code> means these 1000 points will be divided into 5 subsets, which is named as <code>data/set.000</code>~<code>data/set.004</code>, respectively. The size of each set is 200. In these 5 sets, <code>data/set.000</code>~<code>data/set.003</code> will be considered as the training set by the DeePMD-kit, and <code>data/set.004</code> will be considered as the test set. The last set will be considered as the test set by the DeePMD-kit by default. If there is only one set, the set will be both the training set and the test set. (Of course, such test set is meaningless.) </p><h2 id="2-Training"><a href="#2-Training" class="headerlink" title="2. Training"></a>2. Training</h2><p>It&#39;s required to prepare an input script to start the DeePMD-kit training. Are you still out of the fear of being dominated by INCAR script?  Don&#39;t worry, it&#39;s much easier to configure the DeePMD-kit than configuring the VASP. First, let&#39;s download an example and save to <code>input.json</code>:</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https://raw.githubusercontent.com/deepmodeling/deepmd-kit/v1.3.3/examples/water/train/water_se_a.json -O input.json</span><br></pre></td></tr></table></figure><p>The strength of the DeePMD-kit is that the same training parameters are suitable for different systems, so we only need to slightly modify <code>input.json</code> to start training. Here is the first parameter to modify:</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;type_map&quot;</span>:     [<span class="string">&quot;O&quot;</span>, <span class="string">&quot;H&quot;</span>],</span><br></pre></td></tr></table></figure><p>In the DeePMD-kit data, each atom type is numbered as an integer starting from 0. The parameter gives an element name to each atom in the numbering system. Here, we can copy from the content of <code>data/type_map.raw</code>. For example,</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;type_map&quot;</span>:    [<span class="string">&quot;A&quot;</span>, <span class="string">&quot;B&quot;</span>,<span class="string">&quot;C&quot;</span>],</span><br></pre></td></tr></table></figure><p>Next, we are going to modify the neighbour searching parameter:</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;sel&quot;</span>:       [<span class="number">46</span>, <span class="number">92</span>],</span><br></pre></td></tr></table></figure><p>Each number in this list gives the maximum number of atoms of each type among neighbor atoms of an atom. For example, <code>46</code> means there are at most 46 <code>O</code> (type <code>0</code>) neighbours. Here, our elements were modified to <code>A</code>, <code>B</code>, and <code>C</code>, so this parameters is also required to modify. What to do if you don&#39;t know the maximum number of neighbors? You can be roughly estimate one by the density of the system, or try a number blindly. If it is not big enough, the DeePMD-kit will shoot WARNINGS.  Below we changed it to </p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;sel&quot;</span>:       [<span class="number">64</span>, <span class="number">64</span>, <span class="number">64</span>]</span><br></pre></td></tr></table></figure><p>In addtion, we need to modify</p><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;systems&quot;</span>:     [<span class="string">&quot;../data/&quot;</span>],</span><br></pre></td></tr></table></figure><p>to</p><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;systems&quot;</span>:     [<span class="string">&quot;./data/&quot;</span>],</span><br></pre></td></tr></table></figure><p>It is because that the directory to write to is <code>./data/</code> in the current directory. Here I&#39;d like to introduce the definition of the data system. The DeePMD-kit considers that data with corresponding element types and atomic numbers form a system. Our data is generated from a molecular dynamics simulation and meets this condition, so we can put them into one system. Dpdata works the same way. If data cannot be put into a system, multiple systems is required to be set as a list here:</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;training&quot;</span>: &#123;</span><br><span class="line">       <span class="attr">&quot;systems&quot;</span>: [<span class="string">&quot;system1&quot;</span>, <span class="string">&quot;system2&quot;</span>]</span><br></pre></td></tr></table></figure><p>Finnally, we are likely to modify another two parameters:</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;stop_batch&quot;</span>:   <span class="number">1000000</span>,</span><br><span class="line"><span class="string">&quot;batch_size&quot;</span>:   <span class="number">1</span>,</span><br></pre></td></tr></table></figure><p><code>stop_batch</code> is the numebr of training step using the SGD method of deep learning, and <code>batch_size</code> is the mini-batch size of data in each step.<br>If we want to reduce <code>stop_batch</code> and use <code>batch_size</code> that the DeePMD-kit recommends, we can use</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;stop_batch&quot;</span>:   <span class="number">500000</span>,</span><br><span class="line"><span class="string">&quot;batch_size&quot;</span>:   <span class="string">&quot;auto&quot;</span>,</span><br></pre></td></tr></table></figure><p>Now we have succesfully set a input file! To start training, we execuate</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dp train input.json</span><br></pre></td></tr></table></figure><p>and wait for results. During the training process, we can see <code>lcurve.out</code> to observe the error reduction.Among them, Column 4 and 5 are the test and training errors of energy (normalized by the number of atoms), and Column 6 and 7 are the test and training errors of the force. </p><h2 id="3-Freeze-the-Model"><a href="#3-Freeze-the-Model" class="headerlink" title="3. Freeze the Model"></a>3. Freeze the Model</h2><p>After training, we can use the following script to freeze the model:</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dp freeze</span><br></pre></td></tr></table></figure><p>The default filename of the output model is <code>frozen_model.pb</code>. As so, we have got a good or bad DP model. As for the reliability of this model and how to use it, I will give you a detailed tutorial in the next post.</p>]]></content><summary type="html">&lt;p&gt;DeePMD-kit is a software to implement Deep Potential. There is a lot of information on the Internet, but there are not so many tutorials for the new hand, and the official guide is too long. Today, I&amp;#39;ll take you 5 minutes to get started with DeePMD-kit. &lt;/p&gt;
&lt;p&gt;Let&amp;#39;s take a look at the training process of DeePMD-kit:&lt;/p&gt;
&lt;pre class=&quot;mermaid&quot;&gt;
graph LR
A[Prepare data] --&amp;gt; B[Training]
B --&amp;gt; C[Freeze the model]
&lt;/pre&gt;

&lt;p&gt;What? Only three steps? Yes, it&amp;#39;s that simple.</summary><category term="tutorial" scheme="https://deepmodeling.com/blog/categories/tutorial/"/><category term="DeePMD-kit" scheme="https://deepmodeling.com/blog/tags/DeePMD-kit/"/></entry><entry><title>The DeepModeling Manifesto</title><link href="https://deepmodeling.com/blog/manifesto/"/><id>https://deepmodeling.com/blog/manifesto/</id><published>2021-06-09T16:00:00.000Z</published><updated>2021-06-09T16:00:00.000Z</updated><content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>The integration of machine learning and physical modeling is changing the paradigm of scientific research. Those who hope to extend the frontier of science and solve challenging practical problems through computational modeling are coming together in new ways never seen before. This calls for a new infrastructure--new platforms for collaboration, new coding<br>frameworks, new data processing schemes, and new ways of using the computing power.  It also calls for a new culture—the culture of working together closely for the benefit of all, of free exchange and sharing of knowledge and tools, of respect and appreciation of each other&#39;s work, and of the pursuit of harmony among diversity.</p><p>The DeepModeling community is a community of such a group of people.</p><span id="more"></span><h2 id="What-is-DeepModeling"><a href="#What-is-DeepModeling" class="headerlink" title="What is DeepModeling?"></a>What is DeepModeling?</h2><p>The two most important applications of computing are machine learning and physical modeling. The former is an effective tool for analyzing complex data; the latter is a scientific description of the physical world. The vitality boosted by the effective integration of the two is changing all aspects of scientific research. DeepModeling will ultimately be a set of methodologies and tools that combine machine learning, physical modeling, and cutting-edge computational platforms. People who are attracted by the DeepModeling community are attracted by its open, inclusive environment, as well as its dedication to the cause of advancing scientific computing worldwide.</p><h2 id="Why-choose-open-source"><a href="#Why-choose-open-source" class="headerlink" title="Why choose open source?"></a>Why choose open source?</h2><p>There are different interpretations of the term &quot;open source&quot;. The consensus among the DeepModeling community is that open source is a collaborative software development platform based on the spirit of openness and sharing. Open source is a familiar concept for people in the fields of machine learning and computer science, but it is not yet popular in the field of scientific computing. What we advocate is that an algorithm or software should not be judged by the reputation of the journal in which it is published, but by its ability to solve real world problems and its actual contribution to science. The sustainable development of a software requires continuous investment in manpower. It should undergo incremental improvement, and it should be put to the test of solving real-world problems in an open environment. This is often difficult to achieve by individuals or individual groups. The open-source community provides better solutions. </p><h2 id="The-history-of-the-DeepModeling-community"><a href="#The-history-of-the-DeepModeling-community" class="headerlink" title="The history of the DeepModeling community"></a>The history of the DeepModeling community</h2><p>The &quot;DeepModeling Community&quot; started with the initiation of the &quot;deepmd-kit&quot; project. “deepmd-kit&quot; is a software tool that combines machine learning and molecular dynamics, which helps to overcome a long-standing difficulty in the field of molecular dynamics, namely the dilemma of having to choose between efficiency and accuracy. The name &quot;DeepModeling&quot; was proposed by early developers of the deepmd-kit project, with the intention of using deep learning tools to solve the curse of dimensionality problem in multi-scale modeling. DeepModeling has therefore become the name of the GitHub organization (<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2RlZXBtb2RlbGluZw==">https://github.com/deepmodeling<i class="fa fa-external-link-alt"></i></span>) which manages the original deepmd-kit project. After the development of deepmd-kit, the DeepModeling community has successively initiated projects such as dpdata, dp-gen, and dpdispatcher, and extended the modeling scale to electronic structure level through projects such as deepks-kit and ABACUS. These projects have brought together people from all over the world working on molecular simulations. </p><h2 id="The-short-term-plan-and-long-term-vision-of-the-DeepModeling-community"><a href="#The-short-term-plan-and-long-term-vision-of-the-DeepModeling-community" class="headerlink" title="The short-term plan and long-term vision of the DeepModeling community"></a>The short-term plan and long-term vision of the DeepModeling community</h2><p>In the short term, developers in the DeepModeling community will focus on  atomic-scale simulation methods and tools. This includes solving the many-body Schrödinger equation, electronic structure calculation, molecular dynamics simulation, and coarse-grained molecular dynamics simulation. This also includes tasks such as data generation, model training, high-performance optimization, etc. In addition, it includes different workflows and management tools, as well as computing power scheduling tools for different systems, different scenarios, and different purposes. </p><p>It should be pointed out that the combination of physical modeling and machine learning often fundamentally changes the implementation logic of a piece of software. Therefore, the new infrastructure will not be settled once and for all, but will be gradually improved through an iterative process and  upgrades from time to time.</p><p>In the long run, the DeepModeling community is committed to combining physical models at all scales with machine learning methods, using the most cutting-edge computing platforms to solve the most challenging scientific and technological problems faced by the human society.</p><h2 id="How-can-you-contribute"><a href="#How-can-you-contribute" class="headerlink" title="How can you contribute?"></a>How can you contribute?</h2><p> If you want to contribute to an existing project in the DeepModeling community, please just do so or contact<br>the corresponding developer directly; if you want to open a new project in the DeepModeling community, or if you want the DeepModeling community to help develop your project, just contact <span class="exturl" data-url="bWFpbHRvOiYjOTk7JiMxMTE7JiMxMTA7JiN4NzQ7JiN4NjE7JiM5OTsmIzExNjsmIzY0OyYjeDY0OyYjMTAxOyYjMTAxOyYjeDcwOyYjeDZkOyYjeDZmOyYjeDY0OyYjeDY1OyYjMTA4OyYjMTA1OyYjeDZlOyYjMTAzOyYjeDJlOyYjeDZmOyYjMTE0OyYjMTAzOw==">&#99;&#111;&#110;&#x74;&#x61;&#99;&#116;&#64;&#x64;&#101;&#101;&#x70;&#x6d;&#x6f;&#x64;&#x65;&#108;&#105;&#x6e;&#103;&#x2e;&#x6f;&#114;&#103;<i class="fa fa-external-link-alt"></i></span>.</p><p>If you are a programmer who loves science and is attracted by the future scientific computing platform built by the DeepModeling community, you can contribute not only through new algorithms, but also code development specifications, document writing specifications, community databases, task scheduling, workflow management and other tools.  In addition, you can contribute to code architecture design and high-performance optimization tasks in the DeepModeling community. People in the field of scientific computing will greatly appreciate your expertise and contribution.</p><p>If you are a hardcore developer familiar with topics such as electronic structure calculations, molecular dynamics, and finite element methods, the DeepModeling community will be your place to showcase your talents. The addition of machine learning components requires us to rethink about architecture design, each specific implementation for the tasks mentioned above and high-performance optimization. You will become important bridges that connect other developers, contributors, and users in different areas.</p><p>If you have only used some basic scientific software and have worked on some post-processing scripts, the DeepModeling community also needs you. Try to ask questions and communicate on github&#x2F;gitee and other communication platforms, try to give opinions, and try to fork, commit, pr... Your little by little contribution will make the DeepModeling community better and better, and the DeepModeling community will be very grateful for such contributions.</p><p>Even if you are just a bystander, if you support the concept of the DeepModeling community, your recognition and dissemination will also be a great encouragement and support for the DeepModeling community.</p><h2 id="Final-remarks"><a href="#Final-remarks" class="headerlink" title="Final remarks"></a>Final remarks</h2><p>Despite the tremendous advances in AI and computing power, the scientific computing community is largely embedded in an old-fashioned culture. Many of the most important tasks rely on legacy codes. The core algorithms used in many commercial software have been outdated. The self-sufficient style of work is similar to that of the agricultural ages<br>resulting in poor efficiency. It is only in recent years that some promising open-source communities have emerged. However, these communities are often aimed at specific tools for specific scales, and are often maintained by specific academic research groups. They face serious challenges in terms of continuous development and improved user experience.</p><p>The DeepModeling project promises to change all that. </p><p>The combination of machine learning and physical modeling calls for a new paradigm, the open-source community paradigm. Such a paradigm has long been embraced in the computer and electronics industry, with Linux and Andriod being the very well-known examples. In this sense, what the DeepModeling project does is to borrow these ideas and use them for scientific computing. For people in computational science and engineering, efficient and reusable modeling tools that can be continuously improved will free researchers from the plight of no model or with only ad hoc models. For those who work on machine learning, the world of physical models will provide a relatively new and surely vast playground. Working together as an open-source community will make our work more productive, up to date, reliable, and transparent. The spirit of close collaboration, of respect and building on each other’s work will surely inspire more and more people to join the cause of advancing computing for the benefit of the human society. This is an exciting opportunity. This is the future of scientific computing!</p>]]></content><summary type="html">&lt;p&gt;The integration of machine learning and physical modeling is changing the paradigm of scientific research. Those who hope to extend the frontier of science and solve challenging practical problems through computational modeling are coming together in new ways never seen before. This calls for a new infrastructure--new platforms for collaboration, new coding&lt;br&gt;frameworks, new data processing schemes, and new ways of using the computing power.  It also calls for a new culture—the culture of working together closely for the benefit of all, of free exchange and sharing of knowledge and tools, of respect and appreciation of each other&amp;#39;s work, and of the pursuit of harmony among diversity.&lt;/p&gt;
&lt;p&gt;The DeepModeling community is a community of such a group of people.&lt;/p&gt;</summary></entry></feed>